#
# INPUT - Logstash listens on port 8514 for these logs.
#

#input {
#  udp {
#    port => "8514"
#    type => "syslog-cisco"
#  }
#  
#  tcp {
#    port => "8514"
#    type => "syslog-cisco"
#  }
#}

input {

 stdin {
    type => "syslog-cisco-asa"
  }

#  file {
#    type => "syslog-cisco-asa"
#    path => "/srv/logs/*"
#    start_position => "beginning"
#  }

}

# Cisco ASA
filter {
  if [type] == "syslog-cisco-asa" {
    # The switches are sending the same message to all syslog servers for redundancy, this allows us to
    ## only store the message in elasticsearch once by generating a hash of the message and using that as
    ## the document_id.
    fingerprint {
      source              => [ "message" ]
      method              => "SHA1"
      key                 => "Some super secret passphrase for uniqueness."
      concatenate_sources => true
    }

    mutate {
      add_tag => [ "pre-processed", "Firewall", "ASA" ]
    }
    grok {
      match => [
        #"message", "<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp} %{SYSLOGHOST:sysloghost} %%{CISCOTAG:cisco_tag}: %{GREEDYDATA:cisco_message}"
        "message", "%{CISCOTIMESTAMP:timestamp}: %%{CISCOTAG:cisco_tag}: %{GREEDYDATA:cisco_message}"
      ]
    }
    syslog_pri { }
    grok {
      patterns_dir => [ "/usr/share/logstash/cisco-grok-patterns" ]
      match => [
        "cisco_message", "%{CISCOFW104001}",
        "cisco_message", "%{CISCOFW104002}",
        "cisco_message", "%{CISCOFW104003}",
        "cisco_message", "%{CISCOFW104004}",
        "cisco_message", "%{CISCOFW105003}",
        "cisco_message", "%{CISCOFW105004}",
        "cisco_message", "%{CISCOFW105008}",
        "cisco_message", "%{CISCOFW105009}",
        "cisco_message", "%{CISCOFW106001}",
        "cisco_message", "%{CISCOFW106006_106007_106010}",
        "cisco_message", "%{CISCOFW106014}",
        "cisco_message", "%{CISCOFW106015}",
        "cisco_message", "%{CISCOFW106021}",
        "cisco_message", "%{CISCOFW106023}",
        "cisco_message", "%{CISCOFW106100}",
        "cisco_message", "%{CISCOFW304001}",
        "cisco_message", "%{CISCOFW110002}",
        "cisco_message", "%{CISCOFW302010}",
        "cisco_message", "%{CISCOFW302013_302014_302015_302016}",
        "cisco_message", "%{CISCOFW302020_302021}",
        "cisco_message", "%{CISCOFW305011}",
        "cisco_message", "%{CISCOFW313001_313004_313008}",
        "cisco_message", "%{CISCOFW313005}",
        "cisco_message", "%{CISCOFW321001}",
        "cisco_message", "%{CISCOFW402117}",
        "cisco_message", "%{CISCOFW402119}",
        "cisco_message", "%{CISCOFW419001}",
        "cisco_message", "%{CISCOFW419002}",
        "cisco_message", "%{CISCOFW500004}",
        "cisco_message", "%{CISCOFW602303_602304}",
        "cisco_message", "%{CISCOFW710001_710002_710003_710005_710006}",
        "cisco_message", "%{CISCOFW713172}",
        "cisco_message", "%{CISCOFW733100}",
        "cisco_message", "%{WORD:action} %{WORD:protocol} %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}; %{GREEDYDATA:dnssec_validation}",
        "cisco_message", "%{CISCO_ACTION:action} %{WORD:protocol} %{CISCO_REASON:reason}.*(%{IP:src_ip}).*%{IP:dst_ip} on interface %{GREEDYDATA:interface}",
        "cisco_message", "Connection limit exceeded %{INT:inuse_connections}/%{INT:connection_limit} for input packet from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} on interface %{GREEDYDATA:interface}",
        "cisco_message", "TCP Intercept %{DATA:threat_detection} to %{IP:ext_nat_ip}/%{INT:ext_nat_port}.*(%{IP:int_nat_ip}/%{INT:int_nat_port}).*Average rate of %{INT:syn_avg_rate} SYNs/sec exceeded the threshold of %{INT:syn_threshold}.#%{INT}",
        "cisco_message", "Embryonic connection limit exceeded %{INT:econns}/%{INT:limit} for %{WORD:direction} packet from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} on interface %{GREEDYDATA:interface}",
        "cisco_message", "%{MYCISCOFW106023}",
        "cisco_message", "%{MYCISCOFW109012}",
        "cisco_message", "%{MYCISCOFW305003}",
        "cisco_message", "%{MYCISCOFW302001}",
        "cisco_message", "%{MYCISCOFW302002}",

        # This guy added a bunch of grok pattern updates that were never merged to master
        # https://github.com/logstash-plugins/logstash-patterns-core/pull/47/files
        "cisco_message", "%{CISCOFW106001_1}",
        "cisco_message", "%{CISCOFW106001_2}",
        "cisco_message", "%{CISCOFW106006_106007_1}",
        "cisco_message", "%{CISCOFW106006_106007_2}",
        "cisco_message", "%{CISCOFW106010}",
        "cisco_message", "%{CISCOFW302013_302014_302015_302016_1}",
        "cisco_message", "%{CISCOFW302013_302014_302015_302016_2}",
        "cisco_message", "%{CISCOFW302020_302021_1}",
        "cisco_message", "%{CISCOFW302020_302021_2}",
        "cisco_message", "%{CISCOFW602303_602304_1}",
        "cisco_message", "%{CISCOFW602303_602304_2}",
        # Couldn't parse it, log it anyway
        "cisco_message", "%{GREEDYDATA}"
      ]
    }
#    geoip {
#      source => "src_ip"
#      target => "geoip"
#      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
#      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
#    }
#    mutate {
#      convert => [ "[geoip][coordinates]", "float" ]
#    }
    date {
      match => [ "timestamp", "MMM dd HH:mm:ss", "MMM d HH:mm:ss", "MMM dd yyyy HH:mm:ss", "MMM d yyyy HH:mm:ss", "MMM  d HH:mm:ss" ]
      timezone => "America/New_York"
    }
    mutate {
      replace => [ "host", "%{sysloghost}" ]
      add_tag => [ "cisco-asa" ]
    }
  }
}

output {
  #stdout { codec => rubydebug { metadata => true } }

  # Something went wrong with the grok parsing, don't discard the messages though
  if "_grokparsefailure" in [tags] {
    file {
      path => "/tmp/fail-%{type}-%{+YYYY.MM.dd}.log"
    }
  }

  # The message was parsed correctly, and should be sent to elasicsearch.
  if "cisco-asa" in [tags] {
    #file {
    #  path => "/tmp/%{type}-%{+YYYY.MM.dd}.log"
    #}

    elasticsearch {
      hosts           => ["http://instance-1", "http://instance-2", "http://instance-3"]
      ssl             => false
      manage_template => true
      index           => "logstash-cisco-asa-%{+YYYY.MM.dd}"
      document_type   => "%{type}"
      document_id     => "%{fingerprint}"
    }
  }
}
